{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Naive Bayes Classifier</span> \n",
    "\n",
    "\n",
    "Naive Bayes is one of the most practical classification machine learning algorithms. \n",
    "\n",
    "* fast\n",
    "* good performance\n",
    "* simple yet very effective\n",
    "* robust to irrelative features\n",
    "\n",
    "So why is it called naive?\n",
    "\n",
    "Because it does not consider the dependency between features and assume all features are independent of each other which is not the case in reality. This is a naive assumption, hence the name.\n",
    "\n",
    "The accuracy is very good although this naive assumption. A famous example of NB usage is spam filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Example1\n",
    "\n",
    "We assume we have collected the below data for the past 5 days. Based on this data, can we predict if our subject will play in a setting like:\n",
    "\n",
    "    outlook  = overcast\n",
    "    temp     = hot\n",
    "    humidity = normal\n",
    "    windy    = no\n",
    "    \n",
    "<table border=1>\n",
    "  <tr>\n",
    "    <th>Outlook</th>\n",
    "    <th>Temperature</th>\n",
    "    <th>Humidity</th>\n",
    "    <th>Windy</th>\n",
    "    <th>Play</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>sunny</td>\n",
    "    <td>hot</td>\n",
    "    <td>normal</td>\n",
    "    <td>no</td>\n",
    "    <td>yes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>overcast</td>\n",
    "    <td>mild</td>\n",
    "    <td>normal</td>\n",
    "    <td>no</td>\n",
    "    <td>yes</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>rainy</td>\n",
    "    <td>cool</td>\n",
    "    <td>high</td>\n",
    "    <td>yes</td>\n",
    "    <td>no</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>sunny</td>\n",
    "    <td>mild</td>\n",
    "    <td>normal</td>\n",
    "    <td>yes</td>\n",
    "    <td>no</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>overcast</td>\n",
    "    <td>hot</td>\n",
    "    <td>high</td>\n",
    "    <td>no</td>\n",
    "    <td>no</td>\n",
    "  </tr>\n",
    "</table>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to find a representation for our data. We can construct a dictionary to convert stings into numbers and then save them in a dataframe. \n",
    "\n",
    "    outlook: sunny=0, overcast=1, rainy=2\n",
    "    temp: hot=0, mild=1, cool=2\n",
    "    humidity: normal=0, high=1\n",
    "    wind: no=0, yes=1\n",
    "    play: np=0, yes=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'outlook': [0, 1, 2, 0, 1],\n",
    "    'temp'   : [0, 1, 2, 1, 0],\n",
    "    'humid'  : [0, 0, 1, 0, 1],\n",
    "    'wind'   : [0, 0, 1, 1, 0],\n",
    "    'play'   : [1, 1, 0, 0, 0,]    \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use Bayes rule to construct a Naive Bayes classifier. We can write:\n",
    "\n",
    "$$Pr\\left(p|o,t,h,w\\right)\\propto Pr\\left(p\\right)Pr(o|p)Pr(t|p)Pr(h|p)Pr(w|p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate $Pr(p)$ we use marginal probablity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 2), (2, 1)]\n",
      "{0: 0.4, 1: 0.4, 2: 0.2}\n",
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "def marginal_prob(df, column):\n",
    "    '''\n",
    "    Compute the marignal probability for values in a column\n",
    "    '''\n",
    "    # an array contain pairs of (value, count)\n",
    "    vals_counts = [(val, (df[column] == val).sum()) for val in set(df[column])]\n",
    "    print(vals_counts)\n",
    "    total_count = sum([count for val, count in vals_counts])\n",
    "    \n",
    "    # an array contain pairs of (value, probability)\n",
    "    vals_probs = [(val, count/total_count) for val, count in vals_counts]\n",
    "    # a dictionary in which keys are val and values are the corresponding probabilities\n",
    "    return dict(vals_probs)\n",
    "\n",
    "print(marginal_prob(df,'outlook'))\n",
    "\n",
    "print(set(df['outlook']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate probability of a feature given the class (play) we use conditinoal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.49999999750000007, 1: 0.49999999750000007, 2: 4.999999925000001e-09}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conditional_prob(df, feature, c, val):\n",
    "    # c is the class (0 or 1)\n",
    "    df2 = df[df[c] == val][feature]\n",
    "    vals_counts = [[val, (df2 == val).sum() + 1e-8] for val in set(df[feature])]\n",
    "    total_count = sum([count for val, count in vals_counts])\n",
    "    \n",
    "    vals_probs = [(val, count/total_count) for val, count in vals_counts]\n",
    "    return dict(vals_probs)\n",
    "\n",
    "\n",
    "conditional_prob(df,'outlook','play',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use Bayes rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (1, 2)]\n",
      "[(0, 3), (1, 2)]\n",
      "probability of not playing: 0.0689655189536266\n",
      "probability of playing    : 0.9310344810463733\n"
     ]
    }
   ],
   "source": [
    "o = 1\n",
    "t = 0\n",
    "h = 0\n",
    "w = 0\n",
    "\n",
    "c = 0\n",
    "p0 = marginal_prob(df, 'play')[c] * conditional_prob(df, 'outlook', 'play', c)[o] * conditional_prob(df, 'temp', 'play', c)[t] \\\n",
    "* conditional_prob(df, 'humid', 'play', c)[h] * conditional_prob(df, 'wind', 'play', c)[w]\n",
    "\n",
    "c = 1\n",
    "p1 = marginal_prob(df, 'play')[c] * conditional_prob(df, 'outlook', 'play', c)[o] * conditional_prob(df, 'temp', 'play', c)[t] \\\n",
    "* conditional_prob(df, 'humid', 'play', c)[h] * conditional_prob(df, 'wind', 'play', c)[w]\n",
    "\n",
    "# normalizing\n",
    "p_sum = p0 + p1\n",
    "p0 /= p_sum\n",
    "p1 /= p_sum\n",
    "\n",
    "print(\"probability of not playing: {}\".format(p0))\n",
    "print(\"probability of playing    : {}\".format(p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Example 2\n",
    "\n",
    "Suppose we have documents below as our training set. \n",
    "\n",
    "    d1: Chinese Beijing Chinese , class = C\n",
    "    d2: Chinese Chinese Shanghai, class = C\n",
    "    d3: Chinese Macao           , class = C\n",
    "    d4: Tokyo Japan Chinese     , class = J\n",
    "\n",
    "\n",
    "**Exercise:** Train a NB classifier and predict if `d5` belongs to class C or J.\n",
    "\n",
    "    d5: Chinese Chinese Tokyo Japan, class = ?\n",
    "    \n",
    "What about `d6: Chinese Chinese Chinese Tokyo Japan, class = ?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_train_data = {\"d1\": [[\"Chinese\",\"Beijing\",\"Chinese\"], \"C\"],\n",
    "\"d2\": [[\"Chinese\",\"Chinese\",\"Shanghai\"], \"C\"],\n",
    "\"d3\": [[\"Chinese\",\"Macao\"], \"C\"],\n",
    "\"d4\": [[\"Tokyo\",\"Japan\",\"Chinese\"], \"J\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': {'Chinese': 7, 'Beijing': 3, 'Shanghai': 3, 'Macao': 3, 'Japan': 1, 'Tokyo': 1}, 'J': {'Tokyo': 4, 'Japan': 4, 'Chinese': 4, 'Shanghai': 1, 'Beijing': 1, 'Macao': 1}}\n",
      "{'C': 3, 'J': 1}\n"
     ]
    }
   ],
   "source": [
    "word_histogram = {}\n",
    "class_count = {}\n",
    "\n",
    "for key, val in init_train_data.items():\n",
    "    words = val[0]\n",
    "    class_val = val[1]\n",
    "    if class_val in word_histogram:\n",
    "        for i in words:\n",
    "            if i in word_histogram[class_val]:\n",
    "                word_histogram[class_val][i] += 1\n",
    "            else:\n",
    "                word_histogram[class_val][i] = 1\n",
    "        class_count[class_val] += 1\n",
    "    else:\n",
    "        word_histogram[class_val] = {}\n",
    "        for i in words:\n",
    "            if i in word_histogram[class_val]:\n",
    "                word_histogram[class_val][i] += 1\n",
    "            else:\n",
    "                word_histogram[class_val][i] = 1\n",
    "        class_count[class_val] = 1\n",
    "\n",
    "\n",
    "## If some words are not include all the classes, add missing words to those classes with count 1 and increase the other word count \n",
    "word_set = []\n",
    "\n",
    "for i in init_train_data.values():\n",
    "    word_set += i[0]\n",
    "\n",
    "word_set = list(set(word_set))\n",
    "\n",
    "\n",
    "\n",
    "for clss in word_histogram.keys():\n",
    "    not_include_words = []\n",
    "    for word in word_set:\n",
    "        if word not in word_histogram[clss]:\n",
    "            not_include_words.append(word)\n",
    "    for key, val in word_histogram[clss].items():\n",
    "        # 1)  In here, I am adding count of new words to each words that are already available in the class. In this case, both d5 and d6 belongs to class J\n",
    "        #     if need to change, replace as :  word_histogram[clss][key] = val + 1 \n",
    "        # 2)  But instead of adding count of new words, if I add just 1 to already available words in the class, \n",
    "        #     probabilities of C and J change and both d5 and d6 belongs to class C\n",
    "        word_histogram[clss][key] = val + len(not_include_words) \n",
    "    for i in not_include_words:\n",
    "        word_histogram[clss][i] = 1\n",
    "\n",
    "print(word_histogram)\n",
    "print(class_count)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_prob(clss, clss_cnt = class_count):\n",
    "    return clss_cnt[clss]/sum([i for i in clss_cnt.values()])\n",
    "\n",
    "def class_word_prob(word, clss, clss_word_cnt = word_histogram):\n",
    "    return clss_word_cnt[clss][word]/sum([i for i in clss_word_cnt[clss].values()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d5: Chinese Chinese Tokyo Japan, class = ?\n",
    "\n",
    "$$P_C \\propto Pr(C)*Pr(Chinese|C)^2*Pr(Tokyo|C)*Pr(Japan|C) $$\n",
    "$$P_J \\propto Pr(J)*Pr(Chinese|J)^2*Pr(Tokyo|J)*Pr(Japan|J) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of being class C: 0.21686482505647336\n",
      "probability of being class J: 0.7831351749435267\n"
     ]
    }
   ],
   "source": [
    "P_C = class_prob(\"C\")*(class_word_prob(\"Chinese\",\"C\")**2)*class_word_prob(\"Tokyo\",\"C\")*class_word_prob(\"Japan\",\"C\")\n",
    "P_J = class_prob(\"J\")*(class_word_prob(\"Chinese\",\"J\")**2)*class_word_prob(\"Tokyo\",\"J\")*class_word_prob(\"Japan\",\"J\")\n",
    "\n",
    "# Normalize the values\n",
    "\n",
    "p_sum = P_C + P_J\n",
    "\n",
    "P_C = P_C/p_sum\n",
    "P_J = P_J/p_sum\n",
    "\n",
    "print(\"probability of being class C: {}\".format(P_C))\n",
    "print(\"probability of being class J: {}\".format(P_J))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d6: Chinese Chinese Chinese Tokyo Japan, class = ?\n",
    "\n",
    "$$P_C \\propto Pr(C)*Pr(Chinese|C)^3*Pr(Tokyo|C)*Pr(Japan|C) $$\n",
    "$$P_J \\propto Pr(J)*Pr(Chinese|J)^3*Pr(Tokyo|J)*Pr(Japan|J) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of being class C: 0.28766804174786226\n",
      "probability of being class J: 0.7123319582521377\n"
     ]
    }
   ],
   "source": [
    "P_C = class_prob(\"C\")*(class_word_prob(\"Chinese\",\"C\")**3)*class_word_prob(\"Tokyo\",\"C\")*class_word_prob(\"Japan\",\"C\")\n",
    "P_J = class_prob(\"J\")*(class_word_prob(\"Chinese\",\"J\")**3)*class_word_prob(\"Tokyo\",\"J\")*class_word_prob(\"Japan\",\"J\")\n",
    "\n",
    "# Normalize the values\n",
    "\n",
    "p_sum = P_C + P_J\n",
    "\n",
    "P_C = P_C/p_sum\n",
    "P_J = P_J/p_sum\n",
    "\n",
    "print(\"probability of being class C: {}\".format(P_C))\n",
    "print(\"probability of being class J: {}\".format(P_J))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
